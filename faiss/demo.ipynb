{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Real RAG Pipeline: Wikipedia Knowledge Retrieval and Question Answering with FAISS, Sentence Transformers, and Gemini\n",
    "\n",
    "## A Practical End-to-End Example with Langchain and Google's LLM\n",
    "\n",
    "Welcome to this hands-on Jupyter Notebook where we'll build a complete Retrieval Augmented Generation (RAG) pipeline!  We'll take you step-by-step through the process of:\n",
    "\n",
    "1.  **Retrieving Knowledge:** Fetching content from Wikipedia.\n",
    "2.  **Embedding Sentences:** Using Sentence Transformers to create vector representations of our knowledge.\n",
    "3.  **Indexing with FAISS:** Building an efficient index for fast semantic search.\n",
    "4.  **Question Generation (Optional):**  Using Gemini to create example questions (for demonstration).\n",
    "5.  **Question Answering with RAG:**  Using Gemini, Langchain, and our FAISS index to answer questions based on retrieved Wikipedia knowledge.\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment and Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu sentence-transformers wikipedia-api langchain-google-genai python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/workplace/rag-knowledge-graph/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import wikipediaapi\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Knowledge from Wikipedia\n",
    "\n",
    "Wikipedia is an incredible source of information, and for our RAG system, it will serve as our knowledge base. We'll use the `wikipedia-api` library to retrieve the content of a specific article.  Let's choose the \"Artificial Intelligence\" Wikipedia page as our starting point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved Wikipedia article: Artificial Intelligence\n",
      "Extracted 278 sentences from Wikipedia article\n"
     ]
    }
   ],
   "source": [
    "# Initialize Wikipedia API\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "    user_agent='RAG Knowledge Graph Demo/1.0'\n",
    ")\n",
    "\n",
    "# Get Wikipedia article (e.g., \"Artificial Intelligence\")\n",
    "article_title = \"Artificial Intelligence\"\n",
    "page = wiki.page(article_title)\n",
    "\n",
    "if page.exists():\n",
    "    text_content = page.text\n",
    "    print(f\"Successfully retrieved Wikipedia article: {article_title}\")\n",
    "    \n",
    "    # Split content into sentences (using the same approach)\n",
    "    sentences = []\n",
    "    for paragraph in text_content.split(\"\\n\\n\"):\n",
    "        for sentence in paragraph.split(\"\\n\"):\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    print(f\"Extracted {len(sentences)} sentences from Wikipedia article\")\n",
    "else:\n",
    "    print(f\"Article '{article_title}' not found\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sentence Embeddings\n",
    "\n",
    "Sentence embeddings are the key to semantic search. They represent the *meaning* of text as dense vectors in a high-dimensional space.  Sentences with similar meanings will have vectors that are \"close\" to each other in this space. We'll use the `sentence-transformers` library and the `all-MiniLM-L6-v2` model, which provides a good balance of speed and semantic quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (278, 384)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings for all sentences\n",
    "sentence_embeddings = embedder.encode(sentences, convert_to_numpy=True)\n",
    "\n",
    "print(f\"Embeddings shape: {sentence_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a FAISS Index\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library specifically designed for efficient similarity search in large vector datasets. We'll build an **IVF (Inverted File) index with Product Quantization (PQ)**.  Remember from our article, IVF helps to narrow down the search space by clustering vectors, and PQ compresses vectors for memory efficiency and speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 278 points to 8 centroids: please provide at least 312 training points\n",
      "WARNING clustering 278 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 278 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding vectors to index...\n",
      "Index is trained: True\n",
      "Number of vectors in index: 278\n"
     ]
    }
   ],
   "source": [
    "dimension = sentence_embeddings.shape[1]  # Embedding dimension\n",
    "num_vectors = sentence_embeddings.shape[0]  # Number of sentences\n",
    "nlist = 8  # Number of Voronoi cells (inverted lists)\n",
    "M = 2  # Number of sub-quantizers for PQ\n",
    "nbits = 8  # Bits per sub-quantizer\n",
    "\n",
    "# Create the index factory string for IVFPQ\n",
    "index_factory_string = f\"IVF{nlist},PQ{M}x{nbits}\"\n",
    "\n",
    "# Instantiate the FAISS index\n",
    "index = faiss.index_factory(dimension, index_factory_string)\n",
    "index.nprobe = 8\n",
    "\n",
    "print(\"Training index...\")\n",
    "index.train(sentence_embeddings)  # Train on the embeddings\n",
    "\n",
    "print(\"Adding vectors to index...\")\n",
    "index.add(sentence_embeddings)  # Add the embeddings to the index\n",
    "\n",
    "print(f\"Index is trained: {index.is_trained}\")\n",
    "print(f\"Number of vectors in index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Example Questions with Gemini\n",
    "\n",
    "Let's use Google's Gemini model to generate questions based on some randomly selected sentences from our Wikipedia article.  This will give us realistic questions that are grounded in our knowledge base.\n",
    "\n",
    "**Important:** To run this section, you need to have your Google API key set up in a `.env` file as `GOOGLE_API_KEY=YOUR_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected sentences and generated questions:\n",
      "\n",
      "Sentence 150: Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\n",
      "Question 150: According to the sentence, what types of bad actors find artificial intelligence tools useful?\n",
      "\n",
      "Sentence 125: In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US). Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers.\n",
      "Question 125: According to the Wall Street Journal, in what year did big AI companies begin negotiations with US nuclear power providers?\n",
      "\n",
      "Sentence 7: Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\n",
      "Question 7: What concepts were used in methods developed in the late 1980s and 1990s for dealing with uncertain information?\n",
      "\n",
      "Sentence 80: AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).\n",
      "Question 80: What type of officer may oversee the deployment of AI?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your_google_api_key\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Select 4 random sentences with more than 50 characters\n",
    "selected_sentences = {}\n",
    "while len(selected_sentences) < 4:\n",
    "    idx = random.randint(0, len(sentences) - 1)\n",
    "    random_sentence = sentences[idx]\n",
    "    if len(random_sentence) > 50:\n",
    "        selected_sentences[idx] = random_sentence\n",
    "\n",
    "\n",
    "# Generate questions using Gemini\n",
    "question_prompt = \"\"\"\n",
    "For the following sentence, create one specific question that can be answered using only the information in the sentence. \n",
    "Make the question clear and focused.\n",
    "\n",
    "Sentence: {sentence}\n",
    "\n",
    "Generate only the question, without any additional text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "query_texts = []\n",
    "print(\"Selected sentences and generated questions:\\n\")\n",
    "\n",
    "for i, sentence in selected_sentences.items():\n",
    "    response = llm.invoke(question_prompt.format(sentence=sentence))\n",
    "    query_texts.append(response.content.strip())\n",
    "    \n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "    print(f\"Question {i}: {response.content.strip()}\\n\")\n",
    "    \n",
    "    time.sleep(2) # Sleep for 2 seconds to avoid rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search and Question Answering with RAG\n",
    "\n",
    "Here's where the magic happens! We'll take the questions (either the ones generated by Gemini or your own questions), perform a semantic search using our FAISS index to retrieve relevant sentences from Wikipedia, and then use Gemini again to answer the question, *grounded in the retrieved context*.  This is Retrieval Augmented Generation in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching index for query: 'According to the sentence, what types of bad actors find artificial intelligence tools useful?'...\n",
      "\n",
      "Query: According to the sentence, what types of bad actors find artificial intelligence tools useful?\n",
      "\n",
      "Retrieved sentences:\n",
      "\n",
      "Rank 1: Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states. (Distance: 0.4878)\n",
      "Answer: According to the sentence, authoritarian governments, terrorists, criminals, or rogue states find artificial intelligence tools useful.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Searching index for query: 'According to the Wall Street Journal, in what year did big AI companies begin negotiations with US nuclear power providers?'...\n",
      "\n",
      "Query: According to the Wall Street Journal, in what year did big AI companies begin negotiations with US nuclear power providers?\n",
      "\n",
      "Retrieved sentences:\n",
      "\n",
      "Rank 1: In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US). Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers. (Distance: 0.7251)\n",
      "Answer: According to the Wall Street Journal, big AI companies began negotiations with US nuclear power providers in 2024.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Searching index for query: 'What concepts were used in methods developed in the late 1980s and 1990s for dealing with uncertain information?'...\n",
      "\n",
      "Query: What concepts were used in methods developed in the late 1980s and 1990s for dealing with uncertain information?\n",
      "\n",
      "Retrieved sentences:\n",
      "\n",
      "Rank 1: Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. (Distance: 0.7223)\n",
      "Answer: According to the provided context, the methods developed in the late 1980s and 1990s for dealing with uncertain or incomplete information employed concepts from probability and economics.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Searching index for query: 'What type of officer may oversee the deployment of AI?'...\n",
      "\n",
      "Query: What type of officer may oversee the deployment of AI?\n",
      "\n",
      "Retrieved sentences:\n",
      "\n",
      "Rank 1: AI has been used in military operations in Iraq, Syria, Israel and Ukraine. (Distance: 0.7178)\n",
      "Answer: The provided context doesn't specify what type of officer may oversee the deployment of AI. It only states that AI has been used in military operations in certain countries.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Consider the following context from Wikipedia: {sentences}\n",
    "Now answer the following question using only the context provided: {query}?\"\"\"\n",
    "\n",
    "k = 1  # number of nearest neighbors to retrieve\n",
    "\n",
    "for query_text in query_texts:\n",
    "    # Embed the query text\n",
    "    query_embedding = embedder.encode(query_text, convert_to_numpy=True).reshape(\n",
    "        1, -1\n",
    "    )  # reshape to 2D array\n",
    "\n",
    "    print(f\"\\nSearching index for query: '{query_text}'...\")\n",
    "    distances, indices = index.search(query_embedding, k)  # perform the search\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    print(\"\\nQuery:\", query_text)\n",
    "    print(\"\\nRetrieved sentences:\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        print(f\"\\nRank {i+1}: {sentences[idx]} (Distance: {distances[0][i]:.4f})\")\n",
    "        context += sentences[idx] + \"\\n\"\n",
    "\n",
    "    response = llm.invoke(prompt.format(sentences=sentences[idx], query=query_text))\n",
    "    print(f\"Answer: {response.content.strip()}\")\n",
    "        \n",
    "    # separator between queries for better readability\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    time.sleep(2)  # 2 seconds wait between call to avoid rate limiting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
