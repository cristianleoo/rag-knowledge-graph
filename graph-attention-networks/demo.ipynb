{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Attention Networks (GAT) for Document Retrieval\n",
    "===================================================\n",
    "\n",
    "This notebook demonstrates the implementation of Graph Attention Networks (GAT) \n",
    "for learning node embeddings in citation networks, which can be used for \n",
    "document retrieval and relationship analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from typing import Dict, Any, Tuple\n",
    "from torch_geometric.datasets import CoraFull\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from langchain_community.llms import Ollama\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Explore Dataset\n",
    "=======================\n",
    "\n",
    "We use the Cora citation network dataset where:\n",
    "- Nodes represent academic papers\n",
    "- Edges represent citations between papers\n",
    "- Node features are bag-of-words representations of papers\n",
    "- Labels represent paper categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 19793\n",
      "Number of edges: 126842\n",
      "Number of features: 8710\n",
      "Number of classes: 70\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = CoraFull(root='data/CoraFull', transform=NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# Basic info about the dataset\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Number of features: {data.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Attention Network (GAT) Architecture\n",
    "========================================\n",
    "\n",
    "GAT learns node embeddings by attending over node neighborhoods:\n",
    "\n",
    "1. Attention Mechanism:\n",
    "   α_ij = softmax_j(LeakyReLU(a^T [Wh_i || Wh_j]))\n",
    "\n",
    "2. Node Feature Update:\n",
    "   h_i' = σ(∑_j α_ij Wh_j)\n",
    "\n",
    "where:\n",
    "- h_i: features of node i\n",
    "- W: learnable weight matrix\n",
    "- a: learnable attention vector\n",
    "- ||: concatenation\n",
    "- σ: activation function (ELU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GAT layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Second GAT layer to generate final node embeddings\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        return F.normalize(x, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping Implementation\n",
    "===========================\n",
    "\n",
    "Prevents overfitting by monitoring validation loss:\n",
    "- Stops training if loss doesn't improve for 'patience' epochs\n",
    "- Improvement must be greater than 'min_delta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback:\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Evaluation\n",
    "==================\n",
    "\n",
    "Comprehensive evaluation of learned embeddings using:\n",
    "1. Link Prediction (AUC, AP)\n",
    "2. Node Classification\n",
    "3. Embedding Visualization\n",
    "4. Graph Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingEvaluator:\n",
    "    @staticmethod\n",
    "    def compute_metrics(embeddings: torch.Tensor, \n",
    "                       pos_edge_index: torch.Tensor, \n",
    "                       neg_edge_index: torch.Tensor,\n",
    "                       node_labels: torch.Tensor = None) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive evaluation metrics\"\"\"\n",
    "        # Link prediction metrics\n",
    "        pos_score = torch.cosine_similarity(\n",
    "            embeddings[pos_edge_index[0]], embeddings[pos_edge_index[1]])\n",
    "        neg_score = torch.cosine_similarity(\n",
    "            embeddings[neg_edge_index[0]], embeddings[neg_edge_index[1]])\n",
    "        \n",
    "        scores = torch.cat([pos_score, neg_score])\n",
    "        labels = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "        \n",
    "        metrics = {\n",
    "            'auc': roc_auc_score(labels.cpu(), scores.cpu()),\n",
    "            'ap': average_precision_score(labels.cpu(), scores.cpu()),\n",
    "            'avg_pos_sim': pos_score.mean().item(),\n",
    "            'avg_neg_sim': neg_score.mean().item()\n",
    "        }\n",
    "\n",
    "        # Node classification metrics (if labels provided)\n",
    "        if node_labels is not None:\n",
    "            X = embeddings.detach().cpu().numpy()\n",
    "            y = node_labels.cpu().numpy()\n",
    "            \n",
    "            clf = LogisticRegression(max_iter=1000)\n",
    "            cv_scores = cross_val_score(clf, X, y, cv=5)\n",
    "            metrics['node_clf_acc'] = cv_scores.mean()\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_embeddings(embeddings: torch.Tensor, \n",
    "                           labels: torch.Tensor = None,\n",
    "                           perplexity: int = 30) -> None:\n",
    "        \"\"\"Visualize embeddings using t-SNE\"\"\"\n",
    "        embeddings_np = embeddings.detach().cpu().numpy()\n",
    "        \n",
    "        # Create t-SNE visualization\n",
    "        tsne = TSNE(n_components=2, \n",
    "                    perplexity=perplexity, \n",
    "                    random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings_np)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(\n",
    "            embeddings_2d[:, 0], \n",
    "            embeddings_2d[:, 1],\n",
    "            c=labels.cpu().numpy() if labels is not None else None,\n",
    "            cmap='tab10',\n",
    "            alpha=0.6\n",
    "        )\n",
    "        \n",
    "        if labels is not None:\n",
    "            plt.colorbar(scatter, label='Class')\n",
    "        \n",
    "        plt.title('t-SNE Visualization of Node Embeddings')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        \n",
    "        # Add density contours\n",
    "        sns.kdeplot(\n",
    "            x=embeddings_2d[:, 0],\n",
    "            y=embeddings_2d[:, 1],\n",
    "            levels=5,\n",
    "            color='k',\n",
    "            alpha=0.3,\n",
    "            linewidths=1\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('embeddings_tsne.png')\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_similarity_matrix(embeddings: torch.Tensor,\n",
    "                                  labels: torch.Tensor = None) -> None:\n",
    "        \"\"\"Visualize pairwise similarities between embeddings\"\"\"\n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.mm(embeddings, embeddings.t()).cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(sim_matrix, \n",
    "                   cmap='coolwarm', \n",
    "                   center=0,\n",
    "                   square=True)\n",
    "        plt.title('Pairwise Similarity Matrix')\n",
    "        plt.savefig('similarity_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_graph_structure(edge_index: torch.Tensor, \n",
    "                                embeddings: torch.Tensor,\n",
    "                                labels: torch.Tensor = None) -> None:\n",
    "        \"\"\"Visualize graph structure with node colors based on embeddings\"\"\"\n",
    "        G = to_networkx(torch.zeros(embeddings.shape[0]), edge_index)\n",
    "        \n",
    "        # Use t-SNE for color mapping if no labels provided\n",
    "        if labels is None:\n",
    "            colors = TSNE(n_components=1).fit_transform(embeddings.detach().cpu().numpy())\n",
    "        else:\n",
    "            colors = labels.cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw(G, pos, node_color=colors, \n",
    "                node_size=50, cmap='tab10',\n",
    "                with_labels=False, alpha=0.8)\n",
    "        plt.savefig('graph_structure.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(metrics_history: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot all training metrics over time:\n",
    "    - Loss\n",
    "    - AUC-ROC\n",
    "    - Average Precision\n",
    "    - Node Classification Accuracy (if available)\n",
    "    \"\"\"\n",
    "    n_metrics = len([k for k in metrics_history.keys() if metrics_history[k] is not None])\n",
    "    plt.figure(figsize=(5*n_metrics, 4))\n",
    "    \n",
    "    idx = 1\n",
    "    for metric_name, values in metrics_history.items():\n",
    "        if values is not None:\n",
    "            plt.subplot(1, n_metrics, idx)\n",
    "            plt.plot(values)\n",
    "            plt.title(f'{metric_name.replace(\"_\", \" \").title()}')\n",
    "            plt.xlabel('Epoch')\n",
    "            idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Checkpointing\n",
    "==================\n",
    "\n",
    "Save model states and metrics during training:\n",
    "- Regular checkpoints every 10 epochs\n",
    "- Best model based on validation metrics\n",
    "- Complete training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpointer:\n",
    "    def __init__(self, save_dir: str = 'checkpoints'):\n",
    "        self.save_dir = save_dir\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.checkpoint_dir = os.path.join(save_dir, f'run_{self.timestamp}')\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Save metrics history\n",
    "        self.metrics_log = []\n",
    "    \n",
    "    def save_checkpoint(self, \n",
    "                       model: torch.nn.Module, \n",
    "                       epoch: int, \n",
    "                       metrics: dict,\n",
    "                       is_best: bool = False) -> None:\n",
    "        \"\"\"Save model checkpoint and metrics\"\"\"\n",
    "        # Create checkpoint dictionary\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint_path = os.path.join(\n",
    "                self.checkpoint_dir, \n",
    "                f'checkpoint_epoch_{epoch}.pt'\n",
    "            )\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            \n",
    "            # Log metrics\n",
    "            self.metrics_log.append({\n",
    "                'epoch': epoch,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "            # Save metrics log\n",
    "            metrics_path = os.path.join(self.checkpoint_dir, 'metrics_history.pt')\n",
    "            torch.save(self.metrics_log, metrics_path)\n",
    "        \n",
    "        # Save best model separately\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "            torch.save(checkpoint, best_path)\n",
    "    \n",
    "    def load_checkpoint(self, epoch: int = None) -> dict:\n",
    "        \"\"\"Load a specific checkpoint or the best model\"\"\"\n",
    "        if epoch is not None:\n",
    "            checkpoint_path = os.path.join(\n",
    "                self.checkpoint_dir,\n",
    "                f'checkpoint_epoch_{epoch}.pt'\n",
    "            )\n",
    "        else:\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            return torch.load(checkpoint_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No checkpoint found at {checkpoint_path}\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_best_model(model: torch.nn.Module,\n",
    "                       run_dir: str = None) -> Tuple[torch.nn.Module, dict]:\n",
    "        \"\"\"Load the best model from a specific run or the latest run\"\"\"\n",
    "        if run_dir is None:\n",
    "            # Find the latest run\n",
    "            all_runs = sorted(glob.glob('checkpoints/run_*'))\n",
    "            if not all_runs:\n",
    "                raise FileNotFoundError(\"No model runs found\")\n",
    "            run_dir = all_runs[-1]\n",
    "        \n",
    "        best_model_path = os.path.join(run_dir, 'best_model.pt')\n",
    "        if not os.path.exists(best_model_path):\n",
    "            raise FileNotFoundError(f\"No best model found in {run_dir}\")\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        return model, checkpoint['metrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: torch.nn.Module, \n",
    "                  data: Any,\n",
    "                  evaluator: EmbeddingEvaluator) -> dict:\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate embeddings\n",
    "        embeddings = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Generate negative edges for evaluation\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=data.edge_index,\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=data.edge_index.size(1)\n",
    "        )\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = evaluator.compute_metrics(\n",
    "            embeddings,\n",
    "            data.edge_index,\n",
    "            neg_edge_index,\n",
    "            data.y if hasattr(data, 'y') else None\n",
    "        )\n",
    "        \n",
    "        # Visualize results\n",
    "        evaluator.visualize_embeddings(\n",
    "            embeddings,\n",
    "            data.y if hasattr(data, 'y') else None\n",
    "        )\n",
    "        evaluator.visualize_similarity_matrix(embeddings)\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Process\n",
    "===============\n",
    "\n",
    "Main training loop with:\n",
    "1. Edge split for training/validation\n",
    "2. Negative sampling\n",
    "3. Contrastive learning objective\n",
    "4. Regular evaluation and checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat_encoder(model: torch.nn.Module, \n",
    "                     data: Any, \n",
    "                     epochs: int = 100,\n",
    "                     patience: int = 10,\n",
    "                     eval_every: int = 5) -> Tuple[torch.nn.Module, Dict]:\n",
    "    \n",
    "    # Initialize checkpointer\n",
    "    checkpointer = ModelCheckpointer()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    early_stopping = EarlyStoppingCallback(patience=patience)\n",
    "    evaluator = EmbeddingEvaluator()\n",
    "    \n",
    "    # Split edges for training/validation\n",
    "    num_edges = data.edge_index.size(1)\n",
    "    perm = torch.randperm(num_edges)\n",
    "    train_edges = data.edge_index[:, perm[:int(0.8 * num_edges)]]\n",
    "    val_edges = data.edge_index[:, perm[int(0.8 * num_edges):]]\n",
    "    \n",
    "    metrics_history = {\n",
    "        'loss': [], 'auc': [], 'ap': [],\n",
    "        'pos_sim': [], 'neg_sim': []\n",
    "    }\n",
    "    \n",
    "    best_val_metric = float('-inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        embeddings = model(data.x, train_edges)\n",
    "        \n",
    "        # Generate negative samples\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=train_edges,\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=train_edges.size(1)\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        pos_sim = F.cosine_similarity(\n",
    "            embeddings[train_edges[0]], \n",
    "            embeddings[train_edges[1]]\n",
    "        )\n",
    "        neg_sim = F.cosine_similarity(\n",
    "            embeddings[neg_edge_index[0]], \n",
    "            embeddings[neg_edge_index[1]]\n",
    "        )\n",
    "        \n",
    "        loss = F.margin_ranking_loss(\n",
    "            pos_sim,\n",
    "            neg_sim,\n",
    "            torch.ones_like(pos_sim),\n",
    "            margin=0.5\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        metrics_history['loss'].append(loss.item())\n",
    "        \n",
    "        # Evaluation\n",
    "        if epoch % eval_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(data.x, data.edge_index)\n",
    "                neg_edges_eval = negative_sampling(\n",
    "                    edge_index=val_edges,\n",
    "                    num_nodes=data.num_nodes,\n",
    "                    num_neg_samples=val_edges.size(1)\n",
    "                )\n",
    "                \n",
    "                # Compute metrics\n",
    "                metrics = evaluator.compute_metrics(\n",
    "                    embeddings, \n",
    "                    val_edges, \n",
    "                    neg_edges_eval,\n",
    "                    data.y if hasattr(data, 'y') else None\n",
    "                )\n",
    "                \n",
    "                # Update metrics history\n",
    "                for k, v in metrics.items():\n",
    "                    if k in metrics_history:\n",
    "                        metrics_history[k].append(v)\n",
    "                \n",
    "                # Check if this is the best model\n",
    "                val_metric = metrics['auc']\n",
    "                is_best = val_metric > best_val_metric\n",
    "                if is_best:\n",
    "                    best_val_metric = val_metric\n",
    "                \n",
    "                # Save checkpoint\n",
    "                current_metrics = {\n",
    "                    'loss': loss.item(),\n",
    "                    **metrics\n",
    "                }\n",
    "                checkpointer.save_checkpoint(\n",
    "                    model, \n",
    "                    epoch, \n",
    "                    current_metrics,\n",
    "                    is_best=is_best\n",
    "                )\n",
    "                \n",
    "                print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                      f'AUC: {metrics[\"auc\"]:.4f}, AP: {metrics[\"ap\"]:.4f}')\n",
    "                \n",
    "                # Visualizations every 50 epochs\n",
    "                if epoch % 50 == 0:\n",
    "                    evaluator.visualize_embeddings(\n",
    "                        embeddings,\n",
    "                        data.y if hasattr(data, 'y') else None\n",
    "                    )\n",
    "                    evaluator.visualize_similarity_matrix(embeddings)\n",
    "                \n",
    "                if early_stopping(loss.item()):\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    best_checkpoint = checkpointer.load_checkpoint()\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, metrics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAG:\n",
    "    def __init__(self, model: torch.nn.Module, data: Any, k: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize GraphRAG with a trained model and data\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "        \n",
    "        # Generate and store embeddings\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.embeddings = self.model(data.x, data.edge_index)\n",
    "        \n",
    "        # Initialize Ollama\n",
    "        self.llm = Ollama(\n",
    "            model=\"llama3.2:latest\",\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "    def find_similar_nodes(self, query_embedding: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Find k most similar nodes to the query embedding\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Compute similarities with all nodes\n",
    "            similarities = torch.mm(query_embedding, self.embeddings.t())\n",
    "            \n",
    "            # Get top k similar nodes\n",
    "            values, indices = similarities.topk(self.k)\n",
    "            \n",
    "            return indices[0], values[0]\n",
    "\n",
    "    def get_node_context(self, node_idx: int) -> str:\n",
    "        \"\"\"Get context information for a specific node\"\"\"\n",
    "        # Get node features\n",
    "        node_features = self.data.x[node_idx]\n",
    "        \n",
    "        # Find similar nodes\n",
    "        with torch.no_grad():\n",
    "            node_embedding = self.embeddings[node_idx].unsqueeze(0)\n",
    "            similarities = torch.mm(node_embedding, self.embeddings.t())\n",
    "            values, indices = similarities.topk(self.k)\n",
    "        \n",
    "        # Create context string\n",
    "        context_parts = [f\"Node {node_idx} features: {node_features.cpu().numpy().tolist()}\"]\n",
    "        \n",
    "        # Add neighbor information\n",
    "        edge_index = self.data.edge_index.cpu()\n",
    "        neighbors = edge_index[1][edge_index[0] == node_idx].tolist()\n",
    "        if neighbors:\n",
    "            context_parts.append(f\"Connected to nodes: {neighbors}\")\n",
    "        \n",
    "        # Add similar nodes\n",
    "        for idx, sim in zip(indices[0], values[0]):\n",
    "            if idx != node_idx:\n",
    "                context_parts.append(\n",
    "                    f\"Similar node {idx.item()} \"\n",
    "                    f\"(similarity: {sim.item():.3f}): \"\n",
    "                    f\"{self.data.x[idx].cpu().numpy().tolist()}\"\n",
    "                )\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"Query the graph-based RAG system\"\"\"\n",
    "        try:\n",
    "            # Use average embedding as query embedding (simple approach)\n",
    "            with torch.no_grad():\n",
    "                query_embedding = self.embeddings.mean(dim=0, keepdim=True)\n",
    "            \n",
    "            # Find relevant nodes\n",
    "            node_indices, similarities = self.find_similar_nodes(query_embedding)\n",
    "            \n",
    "            # Get context from relevant nodes\n",
    "            contexts = []\n",
    "            for node_idx, sim in zip(node_indices, similarities):\n",
    "                node_context = self.get_node_context(node_idx.item())\n",
    "                contexts.append(f\"Relevance score: {sim.item():.3f}\\n{node_context}\")\n",
    "            \n",
    "            # Combine contexts\n",
    "            combined_context = \"\\n\\n\".join(contexts)\n",
    "            \n",
    "            prompt = f\"\"\"Based on the following graph context:\n",
    "\n",
    "{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please analyze the relationships and patterns in the graph data to answer the question.\n",
    "Focus on the most relevant nodes and their connections.\n",
    "\"\"\"\n",
    "\n",
    "            return self.llm.invoke(prompt)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 0.4989, AUC: 0.9756, AP: 0.9812\n",
      "Epoch 005, Loss: 0.2309, AUC: 0.9607, AP: 0.9610\n",
      "Epoch 010, Loss: 0.1157, AUC: 0.9683, AP: 0.9668\n",
      "Epoch 015, Loss: 0.0772, AUC: 0.9790, AP: 0.9776\n",
      "Epoch 020, Loss: 0.0605, AUC: 0.9833, AP: 0.9822\n",
      "Epoch 025, Loss: 0.0495, AUC: 0.9860, AP: 0.9853\n",
      "Epoch 030, Loss: 0.0454, AUC: 0.9875, AP: 0.9864\n",
      "Epoch 035, Loss: 0.0610, AUC: 0.9875, AP: 0.9875\n",
      "Epoch 040, Loss: 0.0530, AUC: 0.9905, AP: 0.9903\n",
      "Epoch 045, Loss: 0.0354, AUC: 0.9910, AP: 0.9906\n",
      "Epoch 050, Loss: 0.0385, AUC: 0.9920, AP: 0.9917\n",
      "Epoch 055, Loss: 0.1367, AUC: 0.9911, AP: 0.9908\n",
      "Epoch 060, Loss: 0.0486, AUC: 0.9906, AP: 0.9900\n",
      "Epoch 065, Loss: 0.0385, AUC: 0.9909, AP: 0.9905\n",
      "Epoch 070, Loss: 0.0363, AUC: 0.9909, AP: 0.9907\n",
      "Epoch 075, Loss: 0.0345, AUC: 0.9919, AP: 0.9914\n",
      "Epoch 080, Loss: 0.0314, AUC: 0.9931, AP: 0.9926\n",
      "Epoch 085, Loss: 0.0286, AUC: 0.9940, AP: 0.9936\n",
      "Epoch 090, Loss: 0.0455, AUC: 0.9907, AP: 0.9910\n",
      "Epoch 095, Loss: 0.0399, AUC: 0.9917, AP: 0.9913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/l7cwj9sd0c1gqp3xyv7290sm0000gn/T/ipykernel_80253/2731713446.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = GATEncoder(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=32,\n",
    "    out_channels=64\n",
    ")\n",
    "\n",
    "# Train with all enhancements\n",
    "model, metrics = train_gat_encoder(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    epochs=100,\n",
    "    patience=10,\n",
    "    eval_every=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5j/l7cwj9sd0c1gqp3xyv7290sm0000gn/T/ipykernel_74325/166765843.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Load from latest run\n",
    "model = GATEncoder(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=32,\n",
    "    out_channels=64\n",
    ")\n",
    "model, best_metrics = ModelCheckpointer.load_best_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model Metrics:\n",
      "{\n",
      "  \"loss\": 0.028570471331477165,\n",
      "  \"auc\": 0.9940383188258145,\n",
      "  \"ap\": 0.9935596727020666,\n",
      "  \"avg_pos_sim\": 0.924940288066864,\n",
      "  \"avg_neg_sim\": 0.03801451623439789,\n",
      "  \"node_clf_acc\": 0.5499915059118471\n",
      "}\n",
      "\n",
      "Current Evaluation Metrics:\n",
      "{\n",
      "  \"auc\": 0.9943906504193957,\n",
      "  \"ap\": 0.9941193450907881,\n",
      "  \"avg_pos_sim\": 0.9266040921211243,\n",
      "  \"avg_neg_sim\": 0.0395059660077095,\n",
      "  \"node_clf_acc\": 0.5499915059118471\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate loaded model\n",
    "evaluator = EmbeddingEvaluator()\n",
    "evaluation_metrics = evaluate_model(model, data, evaluator)\n",
    "\n",
    "print(\"\\nBest Model Metrics:\")\n",
    "print(json.dumps(best_metrics, indent=2))\n",
    "\n",
    "print(\"\\nCurrent Evaluation Metrics:\")\n",
    "print(json.dumps(evaluation_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After analyzing the graph data, I found that the nodes related to neural networks are:\n",
      "\n",
      "1. **\"Neural Network\"**: This node is connected to several other nodes, including \"Deep Learning\", \"Artificial Intelligence\", and \"Machine Learning\". These connections suggest that neural networks are a key component of these fields.\n",
      "2. **\"Deep Learning\"**: This node is also connected to \"Neural Network\", \"Artificial Intelligence\", and \"Machine Learning\". The connection to \"Neural Network\" implies that deep learning is a subset or application of neural networks.\n",
      "3. **\"Artificial Intelligence\"**: This node is connected to \"Neural Network\", \"Deep Learning\", and \"Machine Learning\". The connections suggest that artificial intelligence is a broader field that encompasses neural networks, deep learning, and machine learning.\n",
      "4. **\"Machine Learning\"**: This node is connected to \"Neural Network\", \"Deep Learning\", and \"Artificial Intelligence\". Like the other nodes, it suggests that machine learning is related to neural networks and deep learning.\n",
      "\n",
      "The relationships between these nodes can be summarized as follows:\n",
      "\n",
      "* Neural Networks are a key component of Deep Learning, Artificial Intelligence, and Machine Learning.\n",
      "* Deep Learning is a subset or application of Neural Networks.\n",
      "* Artificial Intelligence is a broader field that encompasses Neural Networks, Deep Learning, and Machine Learning.\n",
      "* Machine Learning is related to Neural Networks, Deep Learning, and Artificial Intelligence.\n",
      "\n",
      "The graph data suggests that neural networks are at the center of these relationships, with deep learning, artificial intelligence, and machine learning branching out from them.\n"
     ]
    }
   ],
   "source": [
    "# Initialize GraphRAG\n",
    "rag_system = GraphRAG(model, data)\n",
    "\n",
    "# Example query\n",
    "question = \"Find nodes related to neural networks and explain their relationships\"\n",
    "print(rag_system.query(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
